{
    "description": "So you already wanted to tryout some big data, or more precisely, distributed computing tools?Thus,&nbsp;Apache Spark&nbsp;is the one to go. However it can be tedious to setup at the very first time, or sometimes you want to launch it for quick experiments (like a simple REPL).For all these cases, the&nbsp;Spark Notebook&nbsp;is probably what you're looking for.Using Apache Spark, Spark Notebook and&nbsp;Docker, we'll see how to setup a simple environment, execute some analyses and share your work.\n\n\nAuthor:\nandy petrella\nAndy, aka noootsab, is a mathematician that tuned into a distributed computing engineer, mainly in the Geospatial world. When the Big Data age came in, he decided to enjoy it at most and created NextLab, a Big/Smart Data oriented company. Since then, he had fun working for IoT, Genomics, Automotive and Smart cities projects. Building Spark jobs, feeding Cassandra rings and shooting data with machine learning guns. He's also a certified Scala trainer and wrote the Learning Play! Framework 2 book for Packt Publishing.",
    "favorite": "0",
    "length": "39:51",
    "likes": "1",
    "recorded": "2015-04-09",
    "speakers": ["andy-petrella"],
    "tags": [],
    "thumbnail_url": "https://i.ytimg.com/vi/ZoVycBP573A/hqdefault.jpg",
    "title": "15' minutes to setup and try Apache Spark using the Spark Notebook and Docker",
    "videos": [
        {
            "code": "ZoVycBP573A",
            "type": "youtube"
        }
    ],
    "views": "365"
}